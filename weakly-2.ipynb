{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Python\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict\n",
        "\n",
        "# NumPy and PyTorch\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "KmvAwS57pDD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCPy3dGEnOpB"
      },
      "outputs": [],
      "source": [
        "class PropagationPath:\n",
        "    def __init__(self):\n",
        "        self.points = []\n",
        "        self.interaction_types = []   # 0 - initial transmitter (radio antenna)   1 - final point (user)   2 - specular reflection   3 - diffraction around the edge\n",
        "        self.path_gain_db = 0   # dB = 10 * log10(I/I0)   -20 dB = 100 times weaker   -90 dB = 1000000000 times weaker\n",
        "        self.hash = 0   # hashed set of interactions (objects / surfaces)\n",
        "\n",
        "offset = 0\n",
        "def read_int():\n",
        "    global offset, bytes\n",
        "    offset += 4\n",
        "    return bytes[offset-4:offset].copy().view(np.int32)[0]\n",
        "def read_uint():\n",
        "    global offset, bytes\n",
        "    offset += 4\n",
        "    return bytes[offset-4:offset].copy().view(np.uint32)[0]\n",
        "def read_float():\n",
        "    global offset, bytes\n",
        "    offset += 4\n",
        "    return bytes[offset-4:offset].copy().view(np.float32)[0]\n",
        "\n",
        "def read_file(filename: str):\n",
        "    global offset, bytes\n",
        "    offset = 0\n",
        "    bytes = np.fromfile(filename, dtype=np.uint8)\n",
        "\n",
        "    transmitter_count = read_int()\n",
        "    paths = []\n",
        "    for tx in range(transmitter_count):\n",
        "        receiver_count = read_int()\n",
        "        paths.append([])\n",
        "        for rx in range(receiver_count):\n",
        "            path_count = read_int()\n",
        "            paths[tx].append([])\n",
        "            for p in range(path_count):\n",
        "                path = PropagationPath()\n",
        "                point_count = read_int()\n",
        "                for _ in range(point_count):\n",
        "                    x = read_float()\n",
        "                    y = read_float()\n",
        "                    z = read_float()\n",
        "                    path.points.append((x, y, z))\n",
        "                for _ in range(point_count):\n",
        "                    interaction = read_int()\n",
        "                    path.interaction_types.append(interaction)\n",
        "                path.path_gain_db = read_float()\n",
        "                path.hash = read_uint()\n",
        "                paths[tx][rx].append(path)\n",
        "    return paths"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvChain(nn.Module):\n",
        "    \"\"\"A simple stack of convolution layers.\n",
        "\n",
        "    Args:\n",
        "        ninputs(int): number of input channels.\n",
        "        noutputs(int): number of output channels.\n",
        "        ksize(int): size of all the convolution kernels.\n",
        "        width(int): number of channels per intermediate layer.\n",
        "        depth(int): number of intermadiate layers.\n",
        "        stride(int): stride of the convolution.\n",
        "        pad(bool): if True, maintains spatial resolution by 0-padding,\n",
        "            otherwise keep only the valid part.\n",
        "        normalize(bool): applies normalization if True.\n",
        "        normalization_type(str): either batch or instance.\n",
        "        output_type(str): one of linear, relu, leaky_relu, tanh, elu.\n",
        "        activation(str): one of relu, leaky_relu, tanh, elu.\n",
        "        weight_norm(bool): applies weight normalization if True.\n",
        "    \"\"\"\n",
        "    def __init__(self, ninputs, noutputs, ksize=3, width=64, depth=3, stride=1,\n",
        "                 pad=True, normalize=False, normalization_type=\"batch\",\n",
        "                 output_type=\"linear\", activation=\"relu\", weight_norm=True):\n",
        "        super(ConvChain, self).__init__()\n",
        "\n",
        "        if depth <= 0:\n",
        "            LOG.error(\"ConvChain should have non-negative depth.\")\n",
        "            raise ValueError(\"negative network depth.\")\n",
        "\n",
        "        if pad:\n",
        "            padding = ksize//2\n",
        "        else:\n",
        "            padding = 0\n",
        "\n",
        "        layers = []\n",
        "        for d in range(depth-1):\n",
        "            if d == 0:\n",
        "                _in = ninputs\n",
        "            else:\n",
        "                _in = width\n",
        "            layers.append(\n",
        "                ConvChain._ConvBNRelu(_in, ksize, width, normalize=normalize,\n",
        "                                      normalization_type=normalization_type,\n",
        "                                      padding=padding, stride=stride,\n",
        "                                      activation=activation,\n",
        "                                      weight_norm=weight_norm))\n",
        "\n",
        "        # Last layer\n",
        "        if depth > 1:\n",
        "            _in = width\n",
        "        else:\n",
        "            _in = ninputs\n",
        "\n",
        "        conv = nn.Conv2d(_in, noutputs, ksize, bias=True, padding=padding)\n",
        "        if weight_norm:\n",
        "            conv = nn.utils.weight_norm(conv)\n",
        "        conv.bias.data.zero_()\n",
        "        if output_type == \"elu\" or output_type == \"softplus\":\n",
        "            nn.init.xavier_uniform_(\n",
        "                conv.weight.data, nn.init.calculate_gain(\"relu\"))\n",
        "        else:\n",
        "            nn.init.xavier_uniform_(\n",
        "                conv.weight.data, nn.init.calculate_gain(output_type))\n",
        "        layers.append(conv)\n",
        "\n",
        "        # Rename layers\n",
        "        for im, m in enumerate(layers):\n",
        "            if im == len(layers)-1:\n",
        "                name = \"prediction\"\n",
        "            else:\n",
        "                name = \"layer_{}\".format(im)\n",
        "            self.add_module(name, m)\n",
        "\n",
        "        if output_type == \"linear\":\n",
        "            pass\n",
        "        elif output_type == \"relu\":\n",
        "            self.add_module(\"output_activation\", nn.ReLU(inplace=True))\n",
        "        elif output_type == \"leaky_relu\":\n",
        "            self.add_module(\"output_activation\", nn.LeakyReLU(inplace=True))\n",
        "        elif output_type == \"sigmoid\":\n",
        "            self.add_module(\"output_activation\", nn.Sigmoid())\n",
        "        elif output_type == \"tanh\":\n",
        "            self.add_module(\"output_activation\", nn.Tanh())\n",
        "        elif output_type == \"elu\":\n",
        "            self.add_module(\"output_activation\", nn.ELU())\n",
        "        elif output_type == \"softplus\":\n",
        "            self.add_module(\"output_activation\", nn.Softplus())\n",
        "        else:\n",
        "            raise ValueError(\"Unknon output type '{}'\".format(output_type))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for m in self.children():\n",
        "            x = m(x)\n",
        "        return x\n",
        "\n",
        "    class _ConvBNRelu(nn.Module):\n",
        "        \"\"\"Helper class that implements a simple Conv-(Norm)-Activation group.\n",
        "\n",
        "        Args:\n",
        "            ninputs(int): number of input channels.\n",
        "            ksize(int): size of all the convolution kernels.\n",
        "            noutputs(int): number of output channels.\n",
        "            stride(int): stride of the convolution.\n",
        "            pading(int): amount of 0-padding.\n",
        "            normalize(bool): applies normalization if True.\n",
        "            normalization_type(str): either batch or instance.\n",
        "            activation(str): one of relu, leaky_relu, tanh, elu.\n",
        "            weight_norm(bool): if True applies weight normalization.\n",
        "        \"\"\"\n",
        "        def __init__(self, ninputs, ksize, noutputs, normalize=False,\n",
        "                     normalization_type=\"batch\", stride=1, padding=0,\n",
        "                     activation=\"relu\", weight_norm=True):\n",
        "            super(ConvChain._ConvBNRelu, self).__init__()\n",
        "\n",
        "            if activation == \"relu\":\n",
        "                act_fn = nn.ReLU\n",
        "            elif activation == \"leaky_relu\":\n",
        "                act_fn = nn.LeakyReLU\n",
        "            elif activation == \"tanh\":\n",
        "                act_fn = nn.Tanh\n",
        "            elif activation == \"elu\":\n",
        "                act_fn = nn.ELU\n",
        "            else:\n",
        "                LOG.error(\"Incorrect activation %s\", activation)\n",
        "                raise ValueError(\"activation should be one of: \"\n",
        "                                 \"relu, leaky_relu, tanh, elu\")\n",
        "\n",
        "            if normalize:\n",
        "                print(\"nrm\", normalization_type)\n",
        "                conv = nn.Conv2d(ninputs, noutputs, ksize,\n",
        "                                 stride=stride, padding=padding, bias=False)\n",
        "                if normalization_type == \"batch\":\n",
        "                    nrm = nn.BatchNorm2d(noutputs)\n",
        "                elif normalization_type == \"instance\":\n",
        "                    nrm = nn.InstanceNorm2D(noutputs)\n",
        "                else:\n",
        "                    LOG.error(\"Incorrect normalization %s\", normalization_type)\n",
        "                    raise ValueError(\n",
        "                        \"Unkown normalization type {}\".format(\n",
        "                            normalization_type))\n",
        "                nrm.bias.data.zero_()\n",
        "                nrm.weight.data.fill_(1.0)\n",
        "                self.layer = nn.Sequential(conv, nrm, act_fn())\n",
        "            else:\n",
        "                conv = nn.Conv2d(ninputs, noutputs, ksize,\n",
        "                                 stride=stride, padding=padding)\n",
        "                if weight_norm:\n",
        "                    conv = nn.utils.weight_norm(conv)\n",
        "                conv.bias.data.zero_()\n",
        "                self.layer = nn.Sequential(conv, act_fn())\n",
        "\n",
        "            if activation == \"elu\":\n",
        "                nn.init.xavier_uniform_(\n",
        "                    conv.weight.data, nn.init.calculate_gain(\"relu\"))\n",
        "            else:\n",
        "                nn.init.xavier_uniform_(\n",
        "                    conv.weight.data, nn.init.calculate_gain(activation))\n",
        "\n",
        "        def forward(self, x):\n",
        "            out = self.layer(x)\n",
        "            return out\n",
        "\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    \"\"\"A U-net style autoencoder.\n",
        "\n",
        "    Args:\n",
        "        ninputs(int): number of input channels.\n",
        "        noutputs(int): number of output channels.\n",
        "        ksize(int): size of all the convolution kernels.\n",
        "        width(int): number of channels per intermediate layer at the finest\n",
        "            scale.\n",
        "        num_levels(int): number of spatial scales.\n",
        "        num_convs(int): number of conv layers per scale.\n",
        "        max_width(int): max number of features per conv layer.\n",
        "        increase_factor(float): each coarsest scale increases the number of\n",
        "            feature channels by this factor, up to `max_width`.\n",
        "        normalize(bool): applies normalization if True.\n",
        "        normalization_type(str): either batch or instance.\n",
        "        output_type(str): one of linear, relu, leaky_relu, tanh, elu.\n",
        "        activation(str): one of relu, leaky_relu, tanh, elu.\n",
        "    \"\"\"\n",
        "    def __init__(self, ninputs, noutputs, ksize=3, width=64, num_levels=3,\n",
        "                 num_convs=2, max_width=512, increase_factor=1.0,\n",
        "                 normalize=False, normalization_type=\"batch\",\n",
        "                 output_type=\"linear\",\n",
        "                 activation=\"relu\", pooling=\"max\"):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        next_level = None\n",
        "        for lvl in range(num_levels-1, -1, -1):\n",
        "            n_in = min(int(width*(increase_factor)**(lvl-1)), max_width)\n",
        "            w = min(int(width*(increase_factor)**(lvl)), max_width)\n",
        "            n_us = min(int(width*(increase_factor)**(lvl+1)), max_width)\n",
        "            n_out = w\n",
        "            o_type = activation\n",
        "\n",
        "            if lvl == 0:\n",
        "                n_in = ninputs\n",
        "                o_type = output_type\n",
        "                n_out = noutputs\n",
        "            elif lvl == num_levels-1:\n",
        "                n_us = None\n",
        "\n",
        "            next_level = Autoencoder._Level(\n",
        "                n_in, n_out, next_level=next_level, num_us=n_us,\n",
        "                ksize=ksize, width=w, num_convs=num_convs,\n",
        "                output_type=o_type, normalize=normalize,\n",
        "                normalization_type=normalization_type,\n",
        "                activation=activation, pooling=pooling)\n",
        "\n",
        "        self.add_module(\"net\", next_level)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    class _Level(nn.Module):\n",
        "        \"\"\"One scale of the autoencoder processor.\n",
        "\n",
        "        Args:\n",
        "            num_inputs(int): number of input channels.\n",
        "            num_outputs(int): number of output channels.\n",
        "            next_level(Autoencoder._Level or None): the coarser level after\n",
        "                this one, or None if this is the coarsest level.\n",
        "            num_us(int): number of features in the upsampling branch.\n",
        "            ksize(int): size of all the convolution kernels.\n",
        "            width(int): number of channels per intermediate layer at the finest\n",
        "                scale.\n",
        "            num_convs(int): number of conv layers per scale.\n",
        "            output_type(str): one of linear, relu, leaky_relu, tanh, elu.\n",
        "            normalize(bool): applies normalization if True.\n",
        "            normalization_type(str): either batch or instance.\n",
        "            pooling(str): type of downsampling operator: \"max\", \"average\" or\n",
        "                \"conv\".\n",
        "            activation(str): one of relu, leaky_relu, tanh, elu.\n",
        "        \"\"\"\n",
        "        def __init__(self, num_inputs, num_outputs, next_level=None,\n",
        "                     num_us=None, ksize=3, width=64, num_convs=2,\n",
        "                     output_type=\"linear\", normalize=True,\n",
        "                     normalization_type=\"batch\", pooling=\"max\",\n",
        "                     activation=\"relu\"):\n",
        "            super(Autoencoder._Level, self).__init__()\n",
        "\n",
        "            self.is_last = (next_level is None)\n",
        "\n",
        "            if self.is_last:\n",
        "                self.left = ConvChain(\n",
        "                    num_inputs, num_outputs, ksize=ksize, width=width,\n",
        "                    depth=num_convs, stride=1, pad=True,\n",
        "                    normalize=normalize, normalization_type=normalization_type,\n",
        "                    output_type=output_type)\n",
        "            else:\n",
        "                assert num_us is not None\n",
        "\n",
        "                self.left = ConvChain(\n",
        "                    num_inputs, width, ksize=ksize, width=width,\n",
        "                    depth=num_convs, stride=1, pad=True, normalize=normalize,\n",
        "                    normalization_type=normalization_type,\n",
        "                    output_type=activation, activation=activation)\n",
        "                if pooling == \"max\":\n",
        "                    self.downsample = nn.MaxPool2d(2, 2)\n",
        "                elif pooling == \"average\":\n",
        "                    self.downsample = nn.AvgPool2d(2, 2)\n",
        "                elif pooling == \"conv\":\n",
        "                    self.downsample = nn.Conv2d(width, width, 2, stride=2)\n",
        "                else:\n",
        "                    raise ValueError(\"unknown pooling'{}'\".format(pooling))\n",
        "\n",
        "                self.next_level = next_level\n",
        "                self.right = ConvChain(\n",
        "                    num_us + width, num_outputs, ksize=ksize, width=width,\n",
        "                    depth=num_convs, stride=1, pad=True, normalize=normalize,\n",
        "                    normalization_type=normalization_type,\n",
        "                    output_type=output_type)\n",
        "\n",
        "        def forward(self, x):\n",
        "            left = self.left(x)\n",
        "            if self.is_last:\n",
        "                return left\n",
        "\n",
        "            ds = self.downsample(left)\n",
        "            next_level = self.next_level(ds)\n",
        "            us = F.interpolate(\n",
        "                next_level, size=left.shape[-2:], mode='bilinear',\n",
        "                align_corners=False)\n",
        "            # Concat skip connection\n",
        "            concat = th.cat([us, left], 1)\n",
        "            output = self.right(concat)\n",
        "            return output"
      ],
      "metadata": {
        "id": "b-QmIuESrjxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_data(paths, batch_size, ratio):\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    for i in range(len(paths)):\n",
        "      for j in range(len(paths[0])):\n",
        "        if len(paths[i][j]) != 0:\n",
        "          for k in paths[i][j]:\n",
        "            arr = []\n",
        "            c = 0\n",
        "            for l in range(len(k.points)):\n",
        "              c += 1\n",
        "              for m in range(len(k.points[l])):\n",
        "                arr.append(double(k.points[l][m]))\n",
        "            for l in range((5-c)*3, 0, -1):\n",
        "              arr.append(0)\n",
        "            for l in range(len(k.interaction_types)):\n",
        "              arr.append(double(k.interaction_types[l]))\n",
        "            for l in range(5-c, 0, -1):\n",
        "              arr.append(0)\n",
        "            arr.append(k.path_gain_db)\n",
        "            all_data.append(arr)\n",
        "\n",
        "    numpy_data = np.array(all_data)\n",
        "\n",
        "    #rnd_int = np.random.randint(0, len(all_paths))\n",
        "\n",
        "    datasets = {}\n",
        "    datasets['train'] = np.array(numpy_data[:int(len(numpy_data) * ratio)])\n",
        "    datasets['val'] = np.array(numpy_data[int(len(numpy_data) * ratio):])\n",
        "\n",
        "    # Shuffle data\n",
        "    dataset_length = len(datasets['train'])\n",
        "    indices = list(range(dataset_length))\n",
        "    np.random.shuffle(indices)\n",
        "    sampler = torch.utils.data.SubsetRandomSampler(indices)\n",
        "\n",
        "    # Create DataLoaders using these samplers\n",
        "    dataloaders = {\n",
        "        'train': torch.utils.data.DataLoader(\n",
        "            datasets['train'],\n",
        "            batch_size=batch_size,\n",
        "            sampler=sampler,\n",
        "            num_workers=2,\n",
        "            pin_memory=False,\n",
        "        ),\n",
        "        'val': torch.utils.data.DataLoader(\n",
        "            datasets['val'],\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=2,\n",
        "            pin_memory=False,\n",
        "        )\n",
        "    }\n",
        "\n",
        "    return datasets, dataloaders"
      ],
      "metadata": {
        "id": "Doi7hRyKpMHy"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Interface():\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def to_train_mode(self):\n",
        "        for model_name in self.models:\n",
        "            self.models[model_name].train()\n",
        "            assert 'optim_' + model_name in self.optims, '`optim_%s`: an optimization algorithm is not defined.'%(model_name)\n",
        "\n",
        "    def train_batch(self, batch, grad_hook_mode=False):\n",
        "        out_manif = None\n",
        "        self.model.zero_grad()\n",
        "        p_buffer = self.model(batch)\n",
        "        \"\"\" Feature disentanglement \"\"\"\n",
        "        \"\"\"\n",
        "            _, _, c, _, _ = p_buffers['diffuse'].shape\n",
        "            assert c >= 2\n",
        "            if self.disentanglement_option == 'm11r11':\n",
        "                out_manif = p_buffers\n",
        "            elif self.disentanglement_option == 'm10r01':\n",
        "                out_manif = {\n",
        "                        'diffuse': p_buffers['diffuse'][:,:,c//2:,...],\n",
        "                        'specular': p_buffers['specular'][:,:,c//2:,...]\n",
        "                }\n",
        "                p_buffers = {\n",
        "                        'diffuse': p_buffers['diffuse'][:,:,:c//2,...],\n",
        "                        'specular': p_buffers['specular'][:,:,:c//2,...]\n",
        "                }\n",
        "            elif self.disentanglement_option == 'm11r01':\n",
        "                out_manif = p_buffers\n",
        "                p_buffers = {\n",
        "                        'diffuse': p_buffers['diffuse'][:,:,:c//2,...],\n",
        "                        'specular': p_buffers['specular'][:,:,:c//2,...]\n",
        "                }\n",
        "            elif self.disentanglement_option == 'm10r11':\n",
        "                out_manif = {\n",
        "                        'diffuse': p_buffers['diffuse'][:,:,c//2:,...],\n",
        "                        'specular': p_buffers['specular'][:,:,c//2:,...]\n",
        "                }\n",
        "\n",
        "            p_var_diffuse = p_buffers['diffuse'].var(1).mean(1, keepdims=True).detach()\n",
        "            p_var_diffuse /= p_buffers['diffuse'].shape[1] # spp\n",
        "            p_var_specular = p_buffers['specular'].var(1).mean(1, keepdims=True).detach()\n",
        "            p_var_specular /= p_buffers['specular'].shape[1]\n",
        "\n",
        "            # make a new batch\n",
        "            batch = {\n",
        "                'target_total': batch['target_total'],\n",
        "                'target_diffuse': batch['target_diffuse'],\n",
        "                'target_specular': batch['target_specular'],\n",
        "                'kpcn_diffuse_in': torch.cat([batch['kpcn_diffuse_in'], p_buffers['diffuse'].mean(1), p_var_diffuse], 1),\n",
        "                'kpcn_specular_in': torch.cat([batch['kpcn_specular_in'], p_buffers['specular'].mean(1), p_var_specular], 1),\n",
        "                'kpcn_diffuse_buffer': batch['kpcn_diffuse_buffer'],\n",
        "                'kpcn_specular_buffer': batch['kpcn_specular_buffer'],\n",
        "                'kpcn_albedo': batch['kpcn_albedo'],\n",
        "            }\n",
        "\n",
        "        self.models['dncnn'].zero_grad()\n",
        "        out = self._regress_forward(batch)\n",
        "\n",
        "        loss_dict = self._backward(batch, out, out_manif)\n",
        "\n",
        "        if grad_hook_mode: # do not update this model\n",
        "            return\n",
        "\n",
        "        self._logging(loss_dict)\n",
        "\n",
        "        self._optimization()\n",
        "        \"\"\"\n",
        "\n",
        "    def _regress_forward(self, batch):\n",
        "        return self.models['dncnn'](batch)\n",
        "\n",
        "    def _backward(self, batch, out, p_buffers):\n",
        "        assert 'radiance' in out\n",
        "        assert 'diffuse' in out\n",
        "        assert 'specular' in out\n",
        "\n",
        "        total, diffuse, specular = out['radiance'], out['diffuse'], out['specular']\n",
        "        loss_dict = {}\n",
        "        tgt_total = crop_like(batch['target_total'], total)\n",
        "\n",
        "        if self.train_branches: # training diffuse and specular branches\n",
        "            tgt_diffuse = crop_like(batch['target_diffuse'], diffuse)\n",
        "            L_diffuse = self.loss_funcs['l_diffuse'](diffuse, tgt_diffuse)\n",
        "\n",
        "            tgt_specular = crop_like(batch['target_specular'], specular)\n",
        "            L_specular = self.loss_funcs['l_specular'](specular, tgt_specular)\n",
        "\n",
        "            loss_dict['l_diffuse'] = L_diffuse.detach()\n",
        "            loss_dict['l_specular'] = L_specular.detach()\n",
        "\n",
        "            if self.manif_learn:\n",
        "                p_buffer_diffuse = crop_like(p_buffers['diffuse'], diffuse)\n",
        "                L_manif_diffuse = self.loss_funcs['l_manif'](p_buffer_diffuse, tgt_diffuse)\n",
        "                L_diffuse += L_manif_diffuse * self.w_manif\n",
        "\n",
        "                p_buffer_specular = crop_like(p_buffers['specular'], specular)\n",
        "                L_manif_specular = self.loss_funcs['l_manif'](p_buffer_specular, tgt_specular)\n",
        "                L_specular += L_manif_specular * self.w_manif\n",
        "\n",
        "                loss_dict['l_manif_diffuse'] = L_manif_diffuse.detach()\n",
        "                loss_dict['l_manif_specular'] = L_manif_specular.detach()\n",
        "\n",
        "            L_diffuse.backward()\n",
        "            L_specular.backward()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                L_total = self.loss_funcs['l_recon'](total, tgt_total)\n",
        "                loss_dict['l_total'] = L_total.detach()\n",
        "        else: # post-training the entire system\n",
        "            L_total = self.loss_funcs['l_recon'](total, tgt_total)\n",
        "            loss_dict['l_total'] = L_total.detach()\n",
        "            L_total.backward()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            loss_dict['rmse'] = self.loss_funcs['l_test'](total, tgt_total).detach()\n",
        "\n",
        "        return loss_dict\n",
        "\n",
        "    def get_epoch_summary(self, mode, norm):\n",
        "        if mode == 'train':\n",
        "            print('[][][]', end=' ')\n",
        "            for key in self.m_losses:\n",
        "                if key == 'm_val':\n",
        "                    continue\n",
        "                tr_l_tmp = self.m_losses[key] / (norm * 2)\n",
        "                tr_l_tmp *= 1000\n",
        "                print('%s: %.3fE-3'%(key, tr_l_tmp), end='\\t')\n",
        "                self.m_losses[key] = torch.tensor(0.0, device=self.m_losses[key].device)\n",
        "            print('')\n",
        "            return -1.0\n",
        "        else:\n",
        "            return self.m_losses['m_val'].item() / (norm * 2)\n"
      ],
      "metadata": {
        "id": "RPeT3pJqpY3-"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PathNet(nn.Module):\n",
        "    \"\"\"Path embedding network\"\"\"\n",
        "\n",
        "    def __init__(self, ic, intermc=64, outc=3):\n",
        "        super(PathNet, self).__init__()\n",
        "        self.ic = ic\n",
        "        self.intermc = intermc\n",
        "        self.outc = outc\n",
        "        self.final_ic = intermc + intermc\n",
        "\n",
        "        self.embedding = ConvChain(ic, intermc, width=intermc, depth=3,\n",
        "                ksize=1, pad=False)\n",
        "        self.propagation = Autoencoder(intermc, intermc, num_levels=3,\n",
        "                increase_factor=2.0, num_convs=3, width=intermc, ksize=3,\n",
        "                output_type=\"leaky_relu\", pooling=\"max\")\n",
        "        self.final = ConvChain(self.final_ic, outc, width=self.final_ic,\n",
        "                depth=2, ksize=1, pad=False, output_type=\"relu\")\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"PathNet i{}in{}o{}\".format(self.ic, self.intermc, self.outc)\n",
        "\n",
        "    def forward(self, samples):\n",
        "        paths = samples\n",
        "        bs, w = paths.shape\n",
        "\n",
        "        flat = paths.contiguous().view([bs, w])\n",
        "        flat = self.embedding(flat)\n",
        "        print(1111)\n",
        "        flat = flat.view([bs, spp, self.intermc, h, w])\n",
        "        reduced = flat.mean(1)\n",
        "\n",
        "        propagated = self.propagation(reduced)\n",
        "        flat = torch.cat([flat.view([bs*spp, self.intermc, h, w]), propagated.unsqueeze(1).repeat(\n",
        "            [1, spp, 1, 1, 1]).view(bs*spp, self.intermc, h, w)], 1)\n",
        "        out = self.final(flat).view([bs, spp, self.outc, h, w])\n",
        "        return out"
      ],
      "metadata": {
        "id": "RM9Q2Md3pWaO"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_file = 'eu_city_2x2_macro_306.bin'\n",
        "paths = read_file(path_file)\n",
        "\n",
        "# Set random seeds\n",
        "np.random.seed(0)\n",
        "batch_size = 10\n",
        "train_val_ratio = 0.95\n",
        "\n",
        "dataset, dataloaders = init_data(paths, batch_size, train_val_ratio)"
      ],
      "metadata": {
        "id": "LL0fQw5HWfPi"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_in = 1\n",
        "n_out = 1\n",
        "model = PathNet(ic=n_in, outc=n_out)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(\"Using {}.\".format(device))\n",
        "\n",
        "# Send the model to GPU\n",
        "model = model.to(device)\n",
        "\n",
        "interface = Interface(model)\n"
      ],
      "metadata": {
        "id": "wysh3aNyo_vh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d98dcd74-1836-4e2e-d0cd-33e0ec3f69b7"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs in dataloaders['train']:\n",
        "  inputs = inputs.to(device)\n",
        "  print(inputs)\n",
        "  interface.train_batch(inputs, grad_hook_mode=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "_k1R2oqjzv6T",
        "outputId": "e053a8b1-eb53-4e6c-a9e0-64fbdddc94b8"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-135-2f168f2de7c2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_hook_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-45-34c44260d942>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, batch, grad_hook_mode)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mout_manif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mp_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;34m\"\"\" Feature disentanglement \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-131-829a3467e2a4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mflat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mflat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mflat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-38d13dd0ced3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-38d13dd0ced3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1538\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1539\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m             for hook_id, hook in (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (double) and bias type (float) should be the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "params_to_update = model.parameters()\n",
        "print(\"Params to learn:\")\n",
        "for name,param in model.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        print(\"\\t\",name)\n",
        "\n",
        "optimizer = optim.Adam(params_to_update, lr=0.001)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Q4aID2qX0ngk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}