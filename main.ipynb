{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9xV5aniYW6Z"
   },
   "outputs": [],
   "source": [
    "# Supress warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "# Python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NumPy, PyTorch, sklearn\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.manifold import trustworthiness\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Custom\n",
    "from path_loader import PathDataLoader\n",
    "from networks import SiameseNetworkSimple, SiameseNetwork\n",
    "from losses import ContrastiveLoss\n",
    "from patch_generator import PatchGenerator\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRDoWV3cZe8O"
   },
   "outputs": [],
   "source": [
    "def train_siamese_network(data_train, data_val, net, criterion, optimizer, scheduler, val_batch_count, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        for phase in ['val', 'train']:\n",
    "            if phase == 'train':\n",
    "                dataloader = data_train\n",
    "            else:\n",
    "                dataloader = data_val\n",
    "            for i, data in enumerate(dataloader, 0):\n",
    "                input1, input2 , original1, original2, feat1, feat2 = data\n",
    "                output1, output2 = net(input1), net(input2)\n",
    "                #gain as ref\n",
    "                ref1 = input1[:, -1]\n",
    "                ref2 = input2[:, -1]\n",
    "                \n",
    "                #cosine similarity as ref\n",
    "                #cos_sim=cosine_similarity(input1[:,:14],input2[:,:14])\n",
    "                #ref1=1-cos_sim\n",
    "                #ref2= torch.zeros_like(ref1)\n",
    "                \n",
    "                loss = criterion(output1, output2, ref1, ref2)\n",
    "                optimizer.zero_grad()\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            if phase == 'val':\n",
    "                dataloader_object = iter(dataloader)\n",
    "                data = []\n",
    "                for i in range(val_batch_count):\n",
    "                    data_batch = next(dataloader_object)\n",
    "                    data.append(data_batch)\n",
    "                embeddings = []\n",
    "                for i in range(len(data)):\n",
    "                    embeddings_temp1 = net(data[i][0])\n",
    "                    embeddings_temp2 = net(data[i][1])\n",
    "                    embeddings.append(embeddings_temp1)\n",
    "                    embeddings.append(embeddings_temp2)\n",
    "                trust_total = 0\n",
    "                cont_total = 0\n",
    "                for i in range(val_batch_count):\n",
    "                    high_dimensional_distances1 = pairwise_distances(data[i][0], metric='euclidean')\n",
    "                    high_dimensional_distances2 = pairwise_distances(data[i][1], metric='euclidean')\n",
    "\n",
    "                    # Calculate the pairwise distances between points in the low-dimensional embedding space\n",
    "                    low_dimensional_distances1 = pairwise_distances(embeddings[2 * i].detach().numpy(), metric='euclidean')\n",
    "                    low_dimensional_distances2 = pairwise_distances(embeddings[2 * i + 1].detach().numpy(), metric='euclidean')\n",
    "\n",
    "                    # Compute the trustworthiness score\n",
    "                    trustworthiness_score1 = trustworthiness(high_dimensional_distances1, low_dimensional_distances1, n_neighbors=5)\n",
    "                    trustworthiness_score2 = trustworthiness(high_dimensional_distances2, low_dimensional_distances2, n_neighbors=5)\n",
    "\n",
    "                    trust_total += trustworthiness_score1\n",
    "                    trust_total += trustworthiness_score2\n",
    "\n",
    "                    continuity_score1 = np.corrcoef(high_dimensional_distances1.ravel(), low_dimensional_distances1.ravel())[0, 1]\n",
    "                    continuity_score2 = np.corrcoef(high_dimensional_distances1.ravel(), low_dimensional_distances1.ravel())[0, 1]\n",
    "                    \n",
    "                    cont_total += continuity_score1\n",
    "                    cont_total += continuity_score2\n",
    "\n",
    "\n",
    "                print(f\"Phase: {phase}, Epoch: {epoch}, Trustworthiness: {trust_total/(2*val_batch_count)}\")\n",
    "                print(f\"Phase: {phase}, Epoch: {epoch}, Continuity: {cont_total/(2*val_batch_count)}\")\n",
    "                #print(f\"Phase: {phase}, Epoch: {epoch}, Loss: {loss}\")\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(patches, patches_feat, ratio_local, num_pairs = 10000):\n",
    "    num_patches = len(patches)\n",
    "    ratio_nonlocal_pair = (1 - ratio_local) / (1 - (1 / num_patches))\n",
    "    ratio_local_pair = 1 - ratio_nonlocal_pair\n",
    "    \n",
    "    data_pairs = []\n",
    "    while len(data_pairs) < num_pairs:\n",
    "      # Pick local pair or non-local pair\n",
    "      first_patch_index = np.random.randint(num_patches)\n",
    "      second_patch_index = np.random.randint(num_patches)\n",
    "      rnd_local = np.random.uniform(0, 1)\n",
    "      \n",
    "      if rnd_local < ratio_local_pair:\n",
    "        second_patch_index = first_patch_index\n",
    "        \n",
    "      # Pick a random path within a the chosen patch\n",
    "      rnd_1 = np.random.randint(len(patches[first_patch_index]))\n",
    "      rnd_2 = np.random.randint(len(patches[second_patch_index]))\n",
    "    \n",
    "      # In case we get same path\n",
    "      if first_patch_index == second_patch_index:\n",
    "        while rnd_1 == rnd_2:\n",
    "          rnd_2 = np.random.randint(len(patches[second_patch_index]))\n",
    "\n",
    "      data_pairs.append(\n",
    "      (torch.tensor(patches[first_patch_index][rnd_1], dtype=torch.float), \n",
    "      torch.tensor(patches[second_patch_index][rnd_2], dtype=torch.float),\n",
    "      torch.tensor(patches_feat[first_patch_index][rnd_1], dtype=torch.float),\n",
    "      torch.tensor(patches_feat[second_patch_index][rnd_2], dtype=torch.float),\n",
    "      torch.tensor(first_patch_index, dtype=torch.float),\n",
    "      torch.tensor(second_patch_index, dtype=torch.float)))\n",
    "    return data_pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataloaders(data_pairs, train_val_ratio, batch_size):\n",
    "    train_size = int(train_val_ratio * len(data_pairs))\n",
    "    train_data = data_pairs[:train_size]\n",
    "    val_data = data_pairs[train_size:]\n",
    "\n",
    "    dataloaders_train = DataLoader(train_data, batch_size, shuffle=True, drop_last=True)\n",
    "    dataloaders_val = DataLoader(val_data, batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    return dataloaders_train, dataloaders_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(patches):\n",
    "    num_paths_in_patches = []\n",
    "    for i in patches:\n",
    "        num_paths_in_patches.append(len(i))\n",
    "\n",
    "    # Flatten the list -> Normalize every element\n",
    "    flattened_patches = [value for patch in patches for path in patch for value in path]\n",
    "    data_min = min(flattened_patches)\n",
    "    data_max = max(flattened_patches)\n",
    "    normalized_patches = [2 * ((x - data_min) / (data_max - data_min)) - 1 for x in flattened_patches]\n",
    "\n",
    "    # Re-create the 3D list\n",
    "    patches = [[] for i in range(len(num_paths_in_patches))]\n",
    "    c = 0\n",
    "    for i in range(len(num_paths_in_patches)):\n",
    "        for j in range(num_paths_in_patches[i]):\n",
    "            patches[i].append([])\n",
    "            for k in range(21):\n",
    "                patches[i][j].append(normalized_patches[c])\n",
    "                c += 1\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAR6yRplYVun"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "pathLoader = PathDataLoader()\n",
    "paths = pathLoader.read('data/eu_city_2x2_macro_306.bin')\n",
    "#paths = pathLoader.read('data/low_map_paths.bin')\n",
    "#paths = pathLoader.read('data/medium_map_paths.bin')\n",
    "#paths = pathLoader.read('data/high_map_paths.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data\n",
    "batch_size = 256\n",
    "train_val_ratio = 0.95\n",
    "num_patches = 8\n",
    "num_pairs = 10000\n",
    "local_ratio = 0.5\n",
    "val_batch_count = int(num_pairs * (1 - train_val_ratio) / batch_size)\n",
    "\n",
    "#val batch count=4 and batch size=256 gives cleaner tsne plot of path manifold\n",
    "val_batch_count=1\n",
    "\n",
    "# Generate patches\n",
    "gen = PatchGenerator(num_patches, attribute=\"transmitter\")\n",
    "patches = gen.generate_patches(paths)\n",
    "\n",
    "# Transform PathPropagation objects to feature vectors\n",
    "patches_feat = gen.transform_patches(patches)\n",
    "\n",
    "patches_norm = normalize_data(patches_feat)\n",
    "\n",
    "# Generate pairs\n",
    "patches_pairs = generate_pairs(patches_norm, patches_feat, local_ratio, num_pairs)\n",
    "\n",
    "#Create dataloaders\n",
    "dataloader_train, dataloader_val = generate_dataloaders(patches_pairs, train_val_ratio, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpjGuBhwYDfc"
   },
   "outputs": [],
   "source": [
    "# Instantiate the Siamese Network and Loss Function\n",
    "lr=0.00001\n",
    "net = SiameseNetworkSimple()\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.9)   \n",
    "\n",
    "train_siamese_network(dataloader_train, dataloader_val, net, criterion, optimizer, scheduler, val_batch_count, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 to visualize embeddings\n",
    "times_batch = val_batch_count\n",
    "assert val_batch_count >= times_batch\n",
    "\n",
    "dataloader_object = iter(dataloader_val)\n",
    "data = []\n",
    "for i in range(val_batch_count):\n",
    "    data_batch = next(dataloader_object)\n",
    "    data.append(data_batch)\n",
    "\n",
    "embeddings = []\n",
    "color = []\n",
    "path_descriptors=[]\n",
    "for i in range(len(data)):\n",
    "    embeddings_temp1 = net(data[i][0])\n",
    "    embeddings_temp2 = net(data[i][1])\n",
    "    embeddings.append(embeddings_temp1)\n",
    "    embeddings.append(embeddings_temp2)\n",
    "    \n",
    "    #raw path data to be used for comparison using tsne\n",
    "    raw_path_descriptors1= data[i][0]\n",
    "    raw_path_descriptors2 = data[i][1]\n",
    "    path_descriptors.append(raw_path_descriptors1)\n",
    "    path_descriptors.append(raw_path_descriptors2)\n",
    "    \n",
    "    #choose 1 way of visualizing tsne plots below\n",
    "    ###Colour according to gain\n",
    "    \"\"\"\n",
    "    color.append(data[i][0][:,-1])\n",
    "    color.append(data[i][1][:,-1])\n",
    "    tsne_label_text=\"Normalized Gain\"\n",
    "    \"\"\"\n",
    "\n",
    "    ###cossim colouring\n",
    "    \"\"\"\n",
    "    cos_sim=cosine_similarity(data[i][0][:,:14],data[i][1][:,:14])\n",
    "    ref1=cos_sim\n",
    "    ref2=cos_sim\n",
    "    color.append(ref1)\n",
    "    color.append(ref2)\n",
    "    tsne_label_text=\"Cosine Similarity\"\n",
    "    \"\"\"\n",
    "\n",
    "    ###Colour according to patches\n",
    "    color.append(data[i][4])\n",
    "    color.append(data[i][5])\n",
    "    tsne_label_text=\"Discrete Patches\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "mRuG3fKUCHcN",
    "outputId": "314bbfae-86bf-47d1-eda2-2e836a41411d"
   },
   "outputs": [],
   "source": [
    "# Step 2 to visualize embeddings\n",
    "\n",
    "# Convert embeddings to a list\n",
    "embeddings_list = embeddings[0].squeeze().tolist()\n",
    "\n",
    "# Create x-axis indices\n",
    "indices = list(range(len(embeddings_list[0])))\n",
    "\n",
    "# Plot the 1D embeddings\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(len(embeddings_list)):\n",
    "    plt.plot(indices, embeddings_list[i], marker='o', linestyle='-')\n",
    "plt.title('Visualization of 1D Embeddings')\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Embedding Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0, perplexity=25)\n",
    "embed_concat = [embeddings[0]]\n",
    "for i in range(1, len(embeddings)):\n",
    "    embed_concat.append(embeddings[i])\n",
    "torch_concat = torch.cat(embed_concat, dim=0).detach().numpy()\n",
    "\n",
    "X_2d = tsne.fit_transform(torch_concat)\n",
    "color_np = torch.cat(color, dim=0).detach().numpy()\n",
    "\n",
    "\n",
    "trustworthiness_tsne = trustworthiness(torch_concat, X_2d, n_neighbors=5)\n",
    "print(\"t-SNE trustworthiness:\", trustworthiness_tsne)\n",
    "\n",
    "scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c = color_np, cmap='viridis',linewidths=0.05)\n",
    "plt.colorbar(scatter, label=tsne_label_text)\n",
    "plt.title(f't-SNE visualization of 7D data transformed to 2D, Lr={lr}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize raw path descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 to visualize embeddings\n",
    "embeddings=path_descriptors\n",
    "# Convert embeddings to a list\n",
    "embeddings_list = embeddings[0].squeeze().tolist()\n",
    "\n",
    "# Create x-axis indices\n",
    "indices = list(range(len(embeddings_list[0])))\n",
    "\n",
    "# Plot the 1D embeddings\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(len(embeddings_list)):\n",
    "    plt.plot(indices, embeddings_list[i], marker='o', linestyle='-')\n",
    "plt.title('Visualization of 1D Embeddings')\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Embedding Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0, perplexity=25)\n",
    "embed_concat = [embeddings[0]]\n",
    "for i in range(1, len(embeddings)):\n",
    "    embed_concat.append(embeddings[i])\n",
    "torch_concat = torch.cat(embed_concat, dim=0).detach().numpy()\n",
    "\n",
    "X_2d = tsne.fit_transform(torch_concat)\n",
    "color_np = torch.cat(color, dim=0).detach().numpy()\n",
    "\n",
    "\n",
    "trustworthiness_tsne = trustworthiness(torch_concat, X_2d, n_neighbors=5)\n",
    "print(\"t-SNE trustworthiness:\", trustworthiness_tsne)\n",
    "\n",
    "scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c = color_np, cmap='viridis',linewidths=0.05)\n",
    "plt.colorbar(scatter, label=tsne_label_text)\n",
    "plt.title(f't-SNE visualization of raw path(21D) data transformed to 2D, Lr={lr}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write data to files to be used in Blender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE TO FILES\n",
    "\n",
    "val_batch_count = 30\n",
    "\n",
    "file_original_name = 'original_data_high.txt'\n",
    "file_embeddings_name = 'embeddings_high.txt'\n",
    "\n",
    "file_original = open(file_original_name, 'w')\n",
    "file_embeddings = open(file_embeddings_name, 'w')\n",
    "\n",
    "dataloader_object = iter(dataloader_val)\n",
    "\n",
    "with open(file_original_name, 'a') as file:\n",
    "    with open(file_embeddings_name, 'a') as file2:\n",
    "        for i in range(val_batch_count):\n",
    "            data_batch = next(dataloader_object)\n",
    "\n",
    "            embeddings1 = net(data_batch[0])\n",
    "            embeddings2 = net(data_batch[1])\n",
    "\n",
    "            embeddings1_txt = embeddings1.detach().numpy()\n",
    "            embeddings2_txt = embeddings2.detach().numpy()\n",
    "\n",
    "            for row in data_batch[2].numpy():\n",
    "                file.write(' '.join([str(elem) for elem in row]) + '\\n')\n",
    "            for row in data_batch[3].numpy():\n",
    "                file.write(' '.join([str(elem) for elem in row]) + '\\n')\n",
    "            for row in embeddings1_txt:\n",
    "                file2.write(' '.join([str(elem) for elem in row]) + '\\n')\n",
    "            for row in embeddings2_txt:\n",
    "                file2.write(' '.join([str(elem) for elem in row]) + '\\n')\n",
    "\n",
    "file_original.close()\n",
    "file_embeddings.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
