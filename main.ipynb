{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "b9xV5aniYW6Z"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/david/Desktop/dd2430-1/main.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/david/Desktop/dd2430-1/main.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39moptim\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/david/Desktop/dd2430-1/main.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/david/Desktop/dd2430-1/main.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmanifold\u001b[39;00m \u001b[39mimport\u001b[39;00m TSNE\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/david/Desktop/dd2430-1/main.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmanifold\u001b[39;00m \u001b[39mimport\u001b[39;00m trustworthiness\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/david/Desktop/dd2430-1/main.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Custom\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/dd2430-1/myenv/lib/python3.10/site-packages/sklearn/manifold/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mThe :mod:`sklearn.manifold` module implements data embedding techniques.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_isomap\u001b[39;00m \u001b[39mimport\u001b[39;00m Isomap\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_locally_linear\u001b[39;00m \u001b[39mimport\u001b[39;00m LocallyLinearEmbedding, locally_linear_embedding\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_mds\u001b[39;00m \u001b[39mimport\u001b[39;00m MDS, smacof\n",
            "File \u001b[0;32m~/Desktop/dd2430-1/myenv/lib/python3.10/site-packages/sklearn/manifold/_isomap.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecomposition\u001b[39;00m \u001b[39mimport\u001b[39;00m KernelPCA\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpairwise\u001b[39;00m \u001b[39mimport\u001b[39;00m _VALID_METRICS\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneighbors\u001b[39;00m \u001b[39mimport\u001b[39;00m NearestNeighbors, kneighbors_graph, radius_neighbors_graph\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m KernelCenterer\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_param_validation\u001b[39;00m \u001b[39mimport\u001b[39;00m Interval, StrOptions\n",
            "File \u001b[0;32m~/Desktop/dd2430-1/myenv/lib/python3.10/site-packages/sklearn/neighbors/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mThe :mod:`sklearn.neighbors` module implements the k-nearest neighbors\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39malgorithm.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_ball_tree\u001b[39;00m \u001b[39mimport\u001b[39;00m BallTree\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_base\u001b[39;00m \u001b[39mimport\u001b[39;00m VALID_METRICS, VALID_METRICS_SPARSE, sort_graph_by_row_values\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_classification\u001b[39;00m \u001b[39mimport\u001b[39;00m KNeighborsClassifier, RadiusNeighborsClassifier\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:404\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Supress warnings\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "\n",
        "# Python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NumPy, PyTorch, sklearn\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.manifold import trustworthiness\n",
        "\n",
        "# Custom\n",
        "from path_loader import PathDataLoader\n",
        "from networks import SiameseNetworkSimple, SiameseNetworkComplex\n",
        "from losses import ContrastiveLossSimple, ContrastiveLoss\n",
        "from patch_generator import PatchGenerator\n",
        "\n",
        "# Set random seeds\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "def train_siamese_network(train_loader, net, criterion, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            input1, input2, label = data\n",
        "            output1, output2 = net(input1), net(input2)\n",
        "            loss = criterion(output1, output2, label)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if i % 1000 == 0:\n",
        "                print(f\"Epoch {epoch}, Iteration {i}, Loss {loss.item()}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRDoWV3cZe8O"
      },
      "outputs": [],
      "source": [
        "def train_siamese_network2(train_loader, net, criterion, optimizer, epochs=10):\n",
        "    gamma = 1\n",
        "    for epoch in range(epochs):\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            input1, input2 , original1, original2 = data\n",
        "            output1, output2 = net(input1), net(input2)\n",
        "            ref1 = input1[:, -1]\n",
        "            ref2 = input2[:, -1]\n",
        "            loss = criterion(output1, output2, ref1, ref2, gamma)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if i % 1000 == 0:\n",
        "                print(f\"Epoch {epoch}, Iteration {i}, Loss {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAR6yRplYVun"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "pathLoader = PathDataLoader()\n",
        "paths = pathLoader.read('eu_city_2x2_macro_306.bin')[:30000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format data\n",
        "batch_size = 20\n",
        "train_val_ratio = 0.95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "def generate_pairs(patches, ratio_local, num_pairs = 10000):\n",
        "    num_patches = len(patches)\n",
        "    ratio_nonlocal_pair = (1 - ratio_local) / (1 - (1 / num_patches))\n",
        "    ratio_local_pair = 1 - ratio_nonlocal_pair\n",
        "\n",
        "    data_pairs = []\n",
        "    while len(data_pairs) < num_pairs:\n",
        "\n",
        "      # Pick local pair or non-local pair\n",
        "      first_patch_index = np.random.randint(num_patches)\n",
        "      second_patch_index = np.random.randint(num_patches)\n",
        "      rnd_local = np.random.uniform(0, 1)\n",
        "\n",
        "      if rnd_local < ratio_local_pair:\n",
        "        second_patch_index = first_patch_index\n",
        "\n",
        "      # Pick a random path within a the chosen patch\n",
        "      rnd_1 = np.random.randint(len(patches[first_patch_index]))\n",
        "      rnd_2 = np.random.randint(len(patches[second_patch_index]))\n",
        "\n",
        "      # In case we get same path\n",
        "      if first_patch_index == second_patch_index:\n",
        "        while rnd_1 == rnd_2:\n",
        "          rnd_2 = np.random.randint(len(patches[second_patch_index]))\n",
        "\n",
        "      if first_patch_index == second_patch_index:\n",
        "        label = 0\n",
        "      else:\n",
        "        label = 1\n",
        "\n",
        "      data_pairs.append(\n",
        "      (torch.tensor(patches[first_patch_index][rnd_1], dtype=torch.float), \n",
        "      torch.tensor(patches[second_patch_index][rnd_2], dtype=torch.float), \n",
        "      torch.tensor(label, dtype=torch.long)))\n",
        "      \n",
        "    return data_pairs\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_pairs2(patches, patches_feat, ratio_local, num_pairs = 10000):\n",
        "    num_patches = len(patches)\n",
        "    ratio_nonlocal_pair = (1 - ratio_local) / (1 - (1 / num_patches))\n",
        "    ratio_local_pair = 1 - ratio_nonlocal_pair\n",
        "\n",
        "    data_pairs = []\n",
        "    while len(data_pairs) < num_pairs:\n",
        "\n",
        "      # Pick local pair or non-local pair\n",
        "      first_patch_index = np.random.randint(num_patches)\n",
        "      second_patch_index = np.random.randint(num_patches)\n",
        "      rnd_local = np.random.uniform(0, 1)\n",
        "\n",
        "      if rnd_local < ratio_local_pair:\n",
        "        second_patch_index = first_patch_index\n",
        "\n",
        "      # Pick a random path within a the chosen patch\n",
        "      rnd_1 = np.random.randint(len(patches[first_patch_index]))\n",
        "      rnd_2 = np.random.randint(len(patches[second_patch_index]))\n",
        "\n",
        "      # In case we get same path\n",
        "      if first_patch_index == second_patch_index:\n",
        "        while rnd_1 == rnd_2:\n",
        "          rnd_2 = np.random.randint(len(patches[second_patch_index]))\n",
        "\n",
        "\n",
        "      data_pairs.append(\n",
        "      (torch.tensor(patches[first_patch_index][rnd_1], dtype=torch.float), \n",
        "      torch.tensor(patches[second_patch_index][rnd_2], dtype=torch.float),\n",
        "      torch.tensor(patches_feat[first_patch_index][rnd_1], dtype=torch.float),\n",
        "      torch.tensor(patches_feat[second_patch_index][rnd_2], dtype=torch.float)))\n",
        "      \n",
        "    return data_pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_dataloaders(data_pairs, train_val_ratio, batch_size):\n",
        "    train_size = int(train_val_ratio * len(data_pairs))\n",
        "    train_data = data_pairs[:train_size]\n",
        "    val_data = data_pairs[train_size:]\n",
        "\n",
        "    dataloaders_train = DataLoader(train_data, batch_size, shuffle=True)\n",
        "    dataloaders_val = DataLoader(val_data, batch_size, shuffle=True)\n",
        "\n",
        "    return dataloaders_train, dataloaders_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_data(patches):\n",
        "    num_paths_in_patches = []\n",
        "    for i in patches:\n",
        "        num_paths_in_patches.append(len(i))\n",
        "\n",
        "    # Flatten the list -> Normalize every element\n",
        "    flattened_patches = [value for patch in patches for path in patch for value in path]\n",
        "    data_min = min(flattened_patches)\n",
        "    data_max = max(flattened_patches)\n",
        "    normalized_patches = [2 * ((x - data_min) / (data_max - data_min)) - 1 for x in flattened_patches]\n",
        "\n",
        "    # Re-create the 3D list\n",
        "    patches = [[] for i in range(len(num_paths_in_patches))]\n",
        "    c = 0\n",
        "    for i in range(len(num_paths_in_patches)):\n",
        "        for j in range(num_paths_in_patches[i]):\n",
        "            patches[i].append([])\n",
        "            for k in range(21):\n",
        "                patches[i][j].append(normalized_patches[c])\n",
        "                c += 1\n",
        "\n",
        "    return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate patches\n",
        "gen = PatchGenerator(num_patches = 8, attribute=\"transmitter\")\n",
        "patches = gen.generate_patches(paths)\n",
        "\n",
        "# Transform PathPropagation objects to feature vectors\n",
        "patches_feat = gen.transform_patches(patches)\n",
        "\n",
        "patches_norm = normalize_data(patches_feat)\n",
        "\n",
        "# Generate pairs\n",
        "patches_pairs = generate_pairs2(patches_norm, patches_feat, 0.5, 100000)\n",
        "\n",
        "#Create dataloaders\n",
        "dataloader_train, dataloader_val = generate_dataloaders(patches_pairs, train_val_ratio, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpjGuBhwYDfc"
      },
      "outputs": [],
      "source": [
        "# Instantiate the Siamese Network and Loss Function\n",
        "net = SiameseNetworkComplex()\n",
        "criterion = ContrastiveLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHM22PM6acnD",
        "outputId": "30a2f770-f761-4db7-d679-93e118c47645"
      },
      "outputs": [],
      "source": [
        "train_siamese_network2(dataloader_train, net, criterion, optimizer, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WRITE TO FILES\n",
        "times = 100\n",
        "\n",
        "file_original = open('original_data.txt', 'w')\n",
        "file_embeddings = open('embeddings.txt', 'w')\n",
        "\n",
        "dataloader_object = iter(dataloader_val)\n",
        "\n",
        "with open('original_data.txt', 'a') as file:\n",
        "    with open('embeddings.txt', 'a') as file2:\n",
        "        for i in range(times):\n",
        "            data_batch = next(dataloader_object)\n",
        "\n",
        "            embeddings1 = net(data_batch[0])\n",
        "            embeddings2 = net(data_batch[1])\n",
        "\n",
        "            embeddings1_txt = embeddings1.detach().numpy()\n",
        "            embeddings2_txt = embeddings2.detach().numpy()\n",
        "\n",
        "            for row in data_batch[2].numpy():\n",
        "                file.write(' '.join([str(elem) for elem in row]) + '\\n')\n",
        "            for row in data_batch[3].numpy():\n",
        "                file.write(' '.join([str(elem) for elem in row]) + '\\n')\n",
        "            for row in embeddings1_txt:\n",
        "                file2.write(' '.join([str(elem) for elem in row]) + '\\n')\n",
        "            for row in embeddings2_txt:\n",
        "                file2.write(' '.join([str(elem) for elem in row]) + '\\n')\n",
        "\n",
        "file_original.close()\n",
        "file_embeddings.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1 to visualize embeddings\n",
        "dataloader_object = iter(dataloader_val)\n",
        "data_batch = next(dataloader_object)\n",
        "data_batch2 = next(dataloader_object)\n",
        "data_batch3 = next(dataloader_object)\n",
        "data_batch4 = next(dataloader_object)\n",
        "data_batch5 = next(dataloader_object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2 to visualize embeddings\n",
        "print(data_batch[0][1]) # Change second index to see specific path\n",
        "print(data_batch[1][1]) \n",
        "embeddings = net(data_batch[0])\n",
        "embeddings2 = net(data_batch[1])\n",
        "embeddings3 = net(data_batch2[0])\n",
        "embeddings4 = net(data_batch2[1])\n",
        "embeddings5 = net(data_batch3[0])\n",
        "embeddings6 = net(data_batch3[1])\n",
        "embeddings7 = net(data_batch4[0])\n",
        "embeddings8 = net(data_batch4[1])\n",
        "embeddings9 = net(data_batch5[0])\n",
        "embeddings10 = net(data_batch5[1])\n",
        "print(embeddings[0]) # Change index to see specific path\n",
        "print(embeddings2[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trustworthiness_score = trustworthiness(data_batch3[0], embeddings2.detach().numpy(), n_neighbors=8)\n",
        "print(trustworthiness_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "mRuG3fKUCHcN",
        "outputId": "314bbfae-86bf-47d1-eda2-2e836a41411d"
      },
      "outputs": [],
      "source": [
        "# Step 3 to visualize embeddings\n",
        "\n",
        "# Convert embeddings to a list\n",
        "embeddings_list = embeddings.squeeze().tolist()\n",
        "embeddings_list2 = embeddings2.squeeze().tolist()\n",
        "\n",
        "# Create x-axis indices\n",
        "indices = list(range(len(embeddings_list[0])))\n",
        "\n",
        "# Plot the 1D embeddings\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i in range(len(embeddings_list)):\n",
        "    plt.plot(indices, embeddings_list[i], marker='o', linestyle='-')\n",
        "plt.title('Visualization of 1D Embeddings')\n",
        "plt.xlabel('Component')\n",
        "plt.ylabel('Embedding Value')\n",
        "plt.show()\n",
        "\n",
        "# Plot the 1D embeddings\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i in range(len(embeddings_list2)):\n",
        "    plt.plot(indices, embeddings_list2[i], marker='o', linestyle='-')\n",
        "plt.title('Visualization of 1D Embeddings')\n",
        "plt.xlabel('Component')\n",
        "plt.ylabel('Embedding Value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tsne = TSNE(n_components=2, random_state=0, perplexity=10)\n",
        "embed_concat = torch.cat((embeddings, embeddings2, embeddings3, embeddings4, embeddings5, embeddings6, embeddings7, embeddings8, embeddings9, embeddings10), dim=0).detach().numpy()\n",
        "\n",
        "X_2d = tsne.fit_transform(embed_concat)\n",
        "trustworthiness_tsne = trustworthiness(embed_concat, X_2d, n_neighbors=5)\n",
        "print(\"t-SNE trustworthiness:\", trustworthiness_tsne)\n",
        "\n",
        "plt.scatter(X_2d[:, 0], X_2d[:, 1])\n",
        "plt.title('t-SNE visualization of 7D data transformed to 2D')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
