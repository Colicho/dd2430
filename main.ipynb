{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9xV5aniYW6Z"
      },
      "outputs": [],
      "source": [
        "# Supress warnings\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "\n",
        "# Python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NumPy and PyTorch\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Custom\n",
        "from path_loader import PathDataLoader\n",
        "from networks import SiameseNetworkSimple\n",
        "from losses import ContrastiveLossSimple\n",
        "from patch_generator import PatchGenerator\n",
        "\n",
        "# Set random seeds\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRDoWV3cZe8O"
      },
      "outputs": [],
      "source": [
        "def train_siamese_network(train_loader, net, criterion, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            input1, input2, label = data\n",
        "            output1, output2 = net(input1), net(input2)\n",
        "            loss = criterion(output1, output2, label)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if i % 1000 == 0:\n",
        "                print(f\"Epoch {epoch}, Iteration {i}, Loss {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAR6yRplYVun"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "pathLoader = PathDataLoader()\n",
        "paths = pathLoader.read('eu_city_2x2_macro_306.bin')[:10000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format data\n",
        "batch_size = 10\n",
        "train_val_ratio = 0.95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_pairs(patches, ratio_local, num_pairs = 10000):\n",
        "    num_patches = len(patches)\n",
        "    ratio_nonlocal_pair = (1 - ratio_local) / (1 - (1 / num_patches))\n",
        "    ratio_local_pair = 1 - ratio_nonlocal_pair\n",
        "\n",
        "    data_pairs = []\n",
        "    while len(data_pairs) < num_pairs:\n",
        "\n",
        "      # Pick local pair or non-local pair\n",
        "      first_patch_index = np.random.randint(num_patches)\n",
        "      second_patch_index = np.random.randint(num_patches)\n",
        "      rnd_local = np.random.uniform(0, 1)\n",
        "\n",
        "      if rnd_local < ratio_local_pair:\n",
        "        second_patch_index = first_patch_index\n",
        "\n",
        "      # Pick a random path within a the chosen patch\n",
        "      rnd_1 = np.random.randint(len(patches[first_patch_index]))\n",
        "      rnd_2 = np.random.randint(len(patches[second_patch_index]))\n",
        "\n",
        "      # In case we get same path\n",
        "      if first_patch_index == second_patch_index:\n",
        "        while rnd_1 == rnd_2:\n",
        "          rnd_2 = np.random.randint(len(patches[second_patch_index]))\n",
        "\n",
        "      if first_patch_index == second_patch_index:\n",
        "        label = 0\n",
        "      else:\n",
        "        label = 1\n",
        "\n",
        "      data_pairs.append(\n",
        "      (torch.tensor(patches[first_patch_index][rnd_1], dtype=torch.float), \n",
        "      torch.tensor(patches[second_patch_index][rnd_2], dtype=torch.float), \n",
        "      torch.tensor(label, dtype=torch.long)))\n",
        "      \n",
        "    return data_pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_dataloaders(data_pairs, train_val_ratio, batch_size):\n",
        "    train_size = int(train_val_ratio * len(data_pairs))\n",
        "    train_data = data_pairs[:train_size]\n",
        "    val_data = data_pairs[train_size:]\n",
        "\n",
        "    dataloaders_train = DataLoader(train_data, batch_size, shuffle=True)\n",
        "    dataloaders_val = DataLoader(val_data, batch_size, shuffle=True)\n",
        "\n",
        "    return dataloaders_train, dataloaders_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate patches\n",
        "gen = PatchGenerator(num_patches = 8, attribute=\"transmitter\")\n",
        "patches = gen.generate_patches(paths)\n",
        "\n",
        "# Transform PathPropagation objects to normalized feature vectors\n",
        "patches = gen.transform_patches(patches)\n",
        "\n",
        "num_paths_in_patches = []\n",
        "for i in patches:\n",
        "    num_paths_in_patches.append(len(i))\n",
        "flattened_patches = [value for patch in patches for path in patch for value in path]\n",
        "data_min = min(flattened_patches)\n",
        "data_max = max(flattened_patches)\n",
        "normalized_patches = [2 * ((x - data_min) / (data_max - data_min)) - 1 for x in flattened_patches]\n",
        "\n",
        "patches = [[] for i in range(len(num_paths_in_patches))]\n",
        "c = 0\n",
        "for i in range(len(num_paths_in_patches)):\n",
        "    for j in range(num_paths_in_patches[i]):\n",
        "        patches[i].append([])\n",
        "        for k in range(21):\n",
        "            patches[i][j].append(normalized_patches[c])\n",
        "            c += 1\n",
        "\n",
        "# Generate pairs\n",
        "patches_pairs = generate_pairs(patches, 0.5)\n",
        "\n",
        "#Create dataloaders\n",
        "dataloader_train, dataloader_val = generate_dataloaders(patches_pairs, train_val_ratio, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpjGuBhwYDfc"
      },
      "outputs": [],
      "source": [
        "# Instantiate the Siamese Network and Loss Function\n",
        "net = SiameseNetworkSimple()\n",
        "criterion = ContrastiveLossSimple()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHM22PM6acnD",
        "outputId": "30a2f770-f761-4db7-d679-93e118c47645"
      },
      "outputs": [],
      "source": [
        "train_siamese_network(dataloader_train, net, criterion, optimizer, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_iter = iter(dataloader_val)\n",
        "\n",
        "# Retrieve the first element\n",
        "first_batch = next(data_iter)\n",
        "\n",
        "# Access the data from the first batch\n",
        "first_data = first_batch[0]\n",
        "embeddings = net(first_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yes = True\n",
        "while yes:\n",
        "  next_batch = next(data_iter)\n",
        "  for i in next_batch[2]:\n",
        "    print(i)\n",
        "    if i == 0:\n",
        "      yes = False\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(next_batch[2])\n",
        "print(next_batch[0][8])\n",
        "print(next_batch[1][8])\n",
        "embeddings = net(next_batch[0])\n",
        "embeddings2 = net(next_batch[1])\n",
        "print(embeddings[8])\n",
        "print(embeddings2[8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "mRuG3fKUCHcN",
        "outputId": "314bbfae-86bf-47d1-eda2-2e836a41411d"
      },
      "outputs": [],
      "source": [
        "# Convert embeddings to a list\n",
        "embeddings_list = embeddings.squeeze().tolist()\n",
        "embeddings_list2 = embeddings2.squeeze().tolist()\n",
        "print(embeddings_list)\n",
        "\n",
        "# Create x-axis indices\n",
        "indices = list(range(len(embeddings_list)))\n",
        "\n",
        "# Plot the 1D embeddings\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(indices, embeddings_list, marker='o', linestyle='-')\n",
        "plt.title('Visualization of 1D Embeddings')\n",
        "plt.xlabel('Component')\n",
        "plt.ylabel('Embedding Value')\n",
        "plt.show()\n",
        "\n",
        "# Plot the 1D embeddings\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(indices, embeddings_list2, marker='o', linestyle='-')\n",
        "plt.title('Visualization of 1D Embeddings')\n",
        "plt.xlabel('Component')\n",
        "plt.ylabel('Embedding Value')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
